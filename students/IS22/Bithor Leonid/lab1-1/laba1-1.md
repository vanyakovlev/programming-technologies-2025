# Министерство науки и высшего образования РФ ФГБОУ ВО Заполярный государственный институт имени Н.М.Федоровского

## Технологии программирования. Лабораторная работа №1

## Задание

1. Регрессия по теореме универсальной аппроксимации, ручное дифференцирование
2. Бинарная классификация с помощью автодиффиренцирования PyTorch
3. Обучить полносвязную нейронную сеть классификации 3 классов изображений из набора данных CIFAR100 по варианту из примера и затем повысить точность на тестовой выборке.

## Ход работы

### Задание 1

Для выполнения задания 1 необходимо было провести апрроксимацию функции используя средства языка программирования Python и его библиотеки PyTorch. Цель апроксимации функции "угадывание" значения функции в зависимости от входного параметра что является одной из задач обучения нейронных сетей.

Предположительно функция нам не известна, есть лишь коллекция значений типа "вход-выход". Обучение сети для получения аппроксимации функции заключается в нахождении взаимосвязи этих параметров для продолжения построения графика.

Размерность скрытого слоя была выствлена в 1024, что дало уменьшению дельты между значениеми графиков различного количества эпох.

![alt text](Img/1.png)

_Рисунок 1: График зависимости значения вода от выхода сети каждые 10 тысяч итераций_

## Вывод 
На графике видно, что сеть с увеличением числа итераций всё точнее повторяет форму истинной функции, демонстрируя работу теоремы универсальной аппроксимации. Низкое значение финальной средней ошибки подтверждает успешность обучения и точность аппроксимации.

### Задание 2

Бинарная классификация заключается в определнии различия между двумя объектами.
Для примера решения задач использовалась задача XOR.
Автоматическое дифференцирование (autograd) в PyTorch позволяет автоматически вычислять градиенты для всех параметров модели, упрощая процесс реализации обратного распространения ошибки.

![alt text](Img/7.png)

_Рисунок 2: Распределение точек двух классов по правилу XOR_

Однослойный персептрон не способен решить данную задачу, ввиду своей линейности, в отличии от многослойного персептрона.

Процесс обучения заключается в минимизации функции потерь. Данная задача была выполнена, что подтверждает следующее изображение.

![alt text](Img/2.png)

_Рисунок 3: График изменения функции потерь в процессе обучения нейронной сети_

- График отображает быстрое уменьшение ошибки в первые 2,000 итераций с последующей плавной сходимостью. К 10,000 итерациям значение функции потерь стабилизируется на низком уровне, что свидетельствует об успешном обучении модели.

Результат классификации:

![alt text](Img/3.png)

_Рисунок 4: Визуализация разделяющей поверхности нейронной сети для задачи бинарной классификации_

## Вывод 
Модель успешно решила линейно неразделимую задачу XOR с использованием автоматического дифференцирования PyTorch, о чем свидетельствуют корректное распределение точек на рисунке 1 и сходящаяся кривая обучения на рисунке 2, демонстрирующая эффективность алгоритма оптимизации.

### Задание 3

Финальным задание была классификация изображений из выборки CIFAR100.

Пример изображения из выборки CIFAR100:

![alt text](Img/4.png)

Результат обучения классификации образов:

![alt text](Img/5.png)

Сравнение результатов:

![alt text](Img/999.png)

### Вывод

При выполнении лабораторной работы удалось ознакомится с библиотекой PyTorch, позволяющей строить нейронные сети и взаимодействовать с ними. Так же произошло ознакомление с базой данных изображений CIFAR100, которая в отличии от MNIST содержит фотографии в RGB формате.
