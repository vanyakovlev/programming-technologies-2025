# Лабораторная работа №1: Знакомство с mistral ai API. Написание простого текстового ассистента

## Цель работы

Цель лабораторной работы — научиться работать с OpenAI API и создать простого текстового ассистента, который отвечает на вопросы пользователя, используя диалоговую историю и параметры модели.

## Инструменты и настройки

- **Язык программирования**: Python.
- **Библиотеки**:
  - `mistralai` — для работы с API.
  - `dotenv` — для загрузки переменных окружения (например, API-ключ).


## Реализованные задачи

В коде были реализованы следующие задачи:

1. **Использование системного промпта через переменную окружения `.env`**:

    ```python
    import os
    from mistralai import Mistral
    from dotenv import load_dotenv
    
    load_dotenv()
    api_key = os.getenv("api_key")
    temperature = os.getenv("temperature")
    SYSTEM_PROMPT = os.getenv("SYSTEM_PROMPT")
    
    client = Mistral(api_key=api_key)
    
    chat_history = []
    
    if SYSTEM_PROMPT:
        chat_history.append({"role": "system", "content": SYSTEM_PROMPT})

    def get_response(text: str, client: Mistral):
        messages = chat_history.copy() 
    
    response = client.chat.complete(
        model="mistral-tiny",
        messages=messages,
        temperature=float(temperature)
    )
    keepingHistory(response.choices[0].message.content, "assistant")
    return response.choices[0].message.content
    ```

    Переменная SYSTEM_PROMPT извлекается из файла .env с помощью os.getenv("SYSTEM_PROMPT").

    Системный промт: `Ты — инкарнация Доктора из сериала Доктор Кто. Отвечай в его характерной манере: с безудержным энтузиазмом к познанию, метафоричными объяснениями, легким высокомерием гения и искренним состраданием. Не говори о себе в третьем лице. Будь очень краток и остроумен.`

    Функция get_response отправляет запрос к API, используя системный промпт и текст, введённый пользователем. Ответ от модели возвращается и выводится на экран.

    **Результат работы:**

2. **Работа с параметром `temperature`**:

    ```
    temperature = 0.6
    ```

    В процессе работы с языковой моделью был реализован эксперимент с параметром `temperature`, который управляет случайностью выводимых ответов. 

    Я использовал значение температуры 0.1 для получения более сухих, тезисные и аналитические ответы
![img](122.png)
    При температуре 1.3 модель начала генерировать ответы из рандомного набора символов.
 ![img](123.png)  

4. **Ведение истории диалога (контекста переписки)**:

    Так же реализована система сохранения истории диалога, для лучшего понимания контекста общения.
    `main.py`
    ```python
    def keepingHistory(Message: str, role: str):
    global chat_history
    start_index = 1 if SYSTEM_PROMPT else 0
    
    dialog_history = chat_history[start_index:]
    
    if len(dialog_history) >= 6: 
        chat_history.pop(start_index) 
    
    chat_history.append({"role": role, "content": Message})
    ```

    Контекст переписки ограничивался 6 последними сообщениями (3 от пользователя и 3 от ИИ). Это позволяло модели помнить предыдущие вопросы и ответы, улучшая качество взаимодействия и позволяя более точно реагировать на новые запросы пользователя
 ![img](124.png)   


## Вывод

В ходе лабораторной работы был изучен и практически освоен базовый функционал Mistral API. Я подключил API к Python-приложению, реализовал простого текстового ассистента и настроил его поведение с помощью системного промпта, вынесенного в файл .env. Также был проведён эксперимент с параметром temperature, демонстрирующий влияние температуры на стиль и вариативность ответов модели. Для обеспечения контекстного общения была разработана система сохранения и загрузки истории диалога как в оперативной памяти, так и в JSON-файле. В итоге удалось создать работающее консольное приложение-ассистент, поддерживающее диалог, запоминающее контекст и корректно взаимодействующее с Mistral API.
