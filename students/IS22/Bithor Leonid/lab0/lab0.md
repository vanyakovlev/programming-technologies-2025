# Лабораторная работа №0. Тема: Установка локальной модели Qwen.

<ins>Цель</ins>: установить на рабочую машину локальную модель нейросети Qwen и запустить её.

## План

1. Настройка окружения;
2. Запуск языковой модели;
3. Задания.

---

## 1. Настройка окружения
Языковой пакет был уже установлен 

### Установка WebUI
Для работы с языковой моделью была установлена библиотека `text-generation-webui`, которая предоставляет удобный интерфейс для взаимодействия с нейросетями. Следующие шаги были выполнены:

1. Перешел по ссылке на репозиторий `text-generation-webui` на GitHub.
2. Скопировал адрес репозитория и создал папку на своем компьютере для установки.
3. Клонировал репозиторий с помощью команды:
    ```bash
    git clone https://github.com/oobabooga/text-generation-webui
    ```
4. Перешел в директорию репозитория:
    ```bash
    cd text-generation-webui
    ```
5. Создал виртуальную среду и активировал её (для Windows):
    ```bash
    python -m venv venv
    venv\Scripts\activate
    ```
6. Установил все зависимости:
    ```bash
    pip install -r requirements/portable/requirements.txt --upgrade
    ```

После этого также следуя инструкции была выбрана и скачена модель Qwen версии Qwen2.5-Omni-3B-GGUF. В папке models была создана папка Qwen, в которую был помещён скачанный файл.

## _2. Запуск языковой модели_

С помощью команды
```bash
python server.py
```
был запущен WebUI:

![Рисунок 1](1.png)

_Рисунок 1: Запуск WebUI_

В вкладке Model была выбрана скаченная модель Qwen2.5-Omni-3B-f16 (без квантеризации) т.к. видеокарта устройства имеет 8 гб памяти (4060) и с помощью кнопки Load она была успешно загружена.

![Рисунок 2](2.png)

_Рисунок 2. Успешная загрузка модели Qwen_

## _3. Задания_

### Задание №1
Системная подсказка для определенного поведения в чате (в данном случае я создал своего цифрового двойника в deepseek и закинул результат в подсказку)

![Рисунок 3](4.png)

_Рисунок 3. Системная подсказка_

![Рисунок 4](5.png)

_Рисунок 4. Ответ без системной подсказки_

![Рисунок 5](3.png)

_Рисунок 5. Ответ с первой системной подсказкой_

В первом случае модель отвечает кратко.
С системной подсказкой ответ становится более подобным моему цифровому двойнику.

### Задание №2
Далее я решил использовать более слабую модель на 0.6 миллиардов параметров и с квантеризацией Q4_0 (4 бита на параметр). Выбор был обусловлен желанием поработать с моделью для более слабых устройств
Qwen3-0.6B-Q4_0 от unsloth

![Рисунок 6](7.png)

_Рисунок 6. Использование модели Qwen3-0.6B-Q4_0 с включенным режимом мышления_

![Рисунок 7](6.png)

_Рисунок 7. Использование модели Qwen2.5-Omni-3B-f16_

Самое интересное, модели Qwen3 оснащены встроенным «режимом мышления» для улучшения рассуждений и качества отклика.
Этот режим по умолчанию включен, но и его можно отключить добавляя в промту _/no_think_ либо включать включая _/think_

![Рисунок 8](8.png)

_Рисунок 8. Использование модели Qwen3-0.6B-Q4_0 с выключенным режимом мышления_

### Задание №3

Параметр temperature отвечает за уровень случайности в ответе. То есть если выставить низкую temperature (рис. 8), то модель отвечает логично и самыми простыми, очевидными словами. Но если выставить слишком высокую temperature (рис. 9), то модель будет генерировать совсем не связный текст, что видно из примера.

![Рисунок 8](8.png)

_Рисунок 8. Ответ модели при temperature = 0.11_

![Рисунок 9](9.png)

_Рисунок 9. Ответ модели при temperature = 2_

Параметр top_p отвечает за разнообразие слов в ответе. То есть если задать высокое значение top_p = 0.95 (рис. 10), то ответ развёрнутый, с примерами и понятным объяснением. Если задать низкое значение top_p = 0.05 (рис. 11), то модели не хватает разнообразия слов, чтобы корректно дать ответ и она создаёт ошибки.

![Рисунок 10](10.png)

_Рисунок 10. Ответ модели при top_p = 0.95_

![Рисунок 11](11.png)

_Рисунок 11. Ответ модели при top_p = 0.05_

Параметр top_k отвечает за число вариантов слов, которые модель будет рассматривать при ответе. Изменение данного параметра не так явно отражается в ответах модели, но можно сказать, что при top_k = 20 модель отвечает более предсказуемо, при top_k = 0 появляются грамотические ошибки, так как модель не ограничена данным параметром в выборе слов, при top_k=200 высок риск неточностей.

Параметр repetition_penalty штрафует повторяющиеся слова и фразы, снижая вероятность их повторного использования в тексте. При эксперементах с данным параметром, было выявлено, что при repetition_penalty = 1 (рис. 12) модель может повторять некоторые слова в ответе, но ответ остаётся логичным. При значении же repetition_penalty = 1.5 (рис. 13) модель перестаёт отвечать на вопрос корректно и выдаёт не связный текст, который генерируется без остановки.

![Рисунок 12](12.png)

_Рисунок 12. Ответ модели при repetition_penalty = 1_

![Рисунок 13](13.png)

_Рисунок 13. Ответ модели при repetition_penalty = 1.5_

Вывод: В ходе выполнения лабораторной работы было успешно настроено окружение, установлена и запущена языковая модель Qwen. Было выявлено что при исопльзовании системной подсказки модель не всегда корректно следует заданным инструкциям. Проведённое сравнение моделей Qwen и Llama показало, что у них различные подходы к формированию ответов. При проведении эксперементов с параметрами модели, стало ясно, что параметры temperature и repetition_penalty при высоких значениях сильно изменяют генерацию ответа модели, а параметры top_p и top_k не так явно влияют на модель по одиночке. Таким образом, лабораторная работа позволила получить базовое понимание принципов работы с языковыми моделями и их настройками.
