# Лабораторная работа №0. Тема: Установка локальной модели Qwen.

<ins>Цель</ins>: установить на рабочую машину локальную модель нейросети Qwen и запустить её.

## План

1. Настройка окружения;
2. Запуск языковой модели;
3. Задания.

---

## 1. Настройка окружения
Языковой пакет был уже установлен 

### Установка WebUI
Для работы с языковой моделью была установлена библиотека `text-generation-webui`, которая предоставляет удобный интерфейс для взаимодействия с нейросетями. Следующие шаги были выполнены:

1. Перешел по ссылке на репозиторий `text-generation-webui` на GitHub.
2. Скопировал адрес репозитория и создал папку на своем компьютере для установки.
3. Клонировал репозиторий с помощью команды:
    ```bash
    git clone https://github.com/oobabooga/text-generation-webui
    ```
4. Перешел в директорию репозитория:
    ```bash
    cd text-generation-webui
    ```
5. Создал виртуальную среду и активировал её (для Windows):
    ```bash
    python -m venv venv
    venv\Scripts\activate
    ```
6. Установил все зависимости:
    ```bash
    pip install -r requirements/portable/requirements.txt --upgrade
    ```

После этого также следуя инструкции была выбрана и скачена модель Qwen версии Qwen2.5-Omni-3B-GGUF. В папке models была создана папка Qwen, в которую был помещён скачанный файл.

## _2. Запуск языковой модели_

С помощью команды
```bash
python server.py
```
был запущен WebUI:

![Рисунок 1](1.png)

_Рисунок 1: Запуск WebUI_

В вкладке Model была выбрана скаченная модель Qwen2.5-Omni-3B-f16 (без квантеризации) т.к. видеокарта устройства имеет 8 гб памяти (4060) и с помощью кнопки Load она была успешно загружена.

![Рисунок 2](2.png)

_Рисунок 2. Успешная загрузка модели Qwen_

## _3. Задания_

### Задание №1
Системная подсказка для определенного поведения в чате (в данном случае я создал своего цифрового двойника в deepseek и закинул результат в подсказку)

![Рисунок 3](4.png)

_Рисунок 3. Системная подсказка_

![Рисунок 4](5.png)

_Рисунок 4. Ответ без системной подсказки_

![Рисунок 5](3.png)

_Рисунок 5. Ответ с первой системной подсказкой_

В первом случае модель отвечает кратко.
С системной подсказкой ответ становится более подобным моему цифровому двойнику.

### Задание №2
Далее я решил использовать более слабую модель на 0.6 миллиардов параметров и с квантеризацией Q4_0 (4 бита на параметр). Выбор был обусловлен желанием поработать с моделью для более слабых устройств
Qwen3-0.6B-Q4_0 от unsloth

![Рисунок 6](7.png)

_Рисунок 6. Использование модели Qwen3-0.6B-Q4_0 с включенным режимом мышления_

![Рисунок 7](6.png)

_Рисунок 7. Использование модели Qwen2.5-Omni-3B-f16_

Самое интересное, модели Qwen3 оснащены встроенным «режимом мышления» для улучшения рассуждений и качества отклика.
Этот режим по умолчанию включен, но и его можно отключить добавляя в промту _/no_think_ либо включать включая _/think_

![Рисунок 8](8.png)

_Рисунок 8. Использование модели Qwen3-0.6B-Q4_0 с выключенным режимом мышления_

### Задание №3

Параметр temperature отвечает за уровень случайности в ответе. Меняет вероятности всех слов, если параметр имеет малое значение то следующее слово будет более ожидаемым по контексту (логичным) , иначе если параметр больше то следующее слово будет менее ожидаемым по контексту (случайным или креативным, в зависимости от параметра)

![Рисунок 9](9.png)

_Рисунок 8. Ответ модели при temperature = 0.1_

![Рисунок 10](10.png)

_Рисунок 9. Ответ модели при temperature = 3.36_

Top-P ограничивает сам набор слов, из которых нейросеть будет выбирать следующее. Модель смотрит на список возможных следующих слов, отсортированных по вероятности.
Она выбирает только самые вероятные слова из верхушки списка, пока их суммарная вероятность не достигнет заданного порога P.
Все остальные, менее вероятные слова, отбрасываются.

![Рисунок 10](12.png)

_Рисунок 10. Ответ модели при top_p = 0.95_

![Рисунок 11](11.png)

_Рисунок 11. Ответ модели при top_p = 0.05_

Параметр top_k отвечает за число вариантов слов, которые модель будет рассматривать при ответе. Изменение данного параметра не так явно отражается в ответах модели, но можно сказать, что при top_k = 20 модель отвечает более предсказуемо, при top_k = 0 появляются грамотические ошибки, так как модель не ограничена данным параметром в выборе слов, при top_k=200 высок риск неточностей.

Параметр repetition_penalty штрафует повторяющиеся слова и фразы, снижая вероятность их повторного использования в тексте. При эксперементах с данным параметром, было выявлено, что при repetition_penalty = 1 (рис. 12) модель может повторять некоторые слова в ответе, но ответ остаётся логичным. При значении же repetition_penalty = 1.5 (рис. 13) модель перестаёт отвечать на вопрос корректно и выдаёт не связный текст, который генерируется без остановки.

![Рисунок 12](12.png)

_Рисунок 12. Ответ модели при repetition_penalty = 1_

![Рисунок 13](13.png)

_Рисунок 13. Ответ модели при repetition_penalty = 1.5_

### Вывод: В ходе выполнения лабораторной работы было успешно настроено окружение, установлена и запущена языковая модель Qwen. Проведённое сравнение моделей разных поколений и функций Qwen 2.5 и Qwen 3 (С режимом мышления) показало, что у них различные подходы к формированию ответов. При проведении эксперементов с параметрами модели, стало ясно, что параметры temperature и repetition_penalty при высоких значениях сильно изменяют генерацию ответа модели, а параметры top_p и top_k не так явно влияют на модель по одиночке. Таким образом, лабораторная работа позволила получить базовое понимание принципов работы с языковыми моделями и их настройками.
