### Лабораторная работа №2. Сверточная нейросеть

**Задания:**

**`1.`**  Взять свои классы из прошлой лабораторной работы и обучить сверточную сеть используя GPU.

**`2.`**  Повысить точность модели, проведя три обучения для 3 разных тактик пуллинга.

### Часть 1. 

**Свёрточная нейросеть (CNN)** — это нейросеть, которая хорошо подходит для работы с изображениями, потому что она умеет находить локальные признаки: линии, края, текстуры, а затем собирать из них более сложные элементы (части объектов) и в итоге определять, что изображено на картинке.

**Основной элемент CNN — свёртка.** В ней используется ядро **(kernel, фильтр)** — небольшая матрица чисел, например 3×3 или 5×5. Ядро “скользит” по изображению: в каждой позиции оно умножает свои значения на соответствующие пиксели, суммирует результат и получает одно число. Так формируется карта признаков — новая “картинка”, где усилены нужные детали (например, границы). Значения ядра не задаются вручную: сеть обучается и сама подбирает фильтры, которые лучше всего помогают решать задачу.

Для того чтобы нейросеть обучалась быстрее нам нужно было использовать GPU.

![GPU](Screen/1.png)

_Рисунок 1: Смена апаратного ускорителя на GPU_

**Архитектура модели**

- `Вход: изображение 32×32×3 (NHWC)`
- `Свёрточный слой 1: Conv2d(3 → 32), kernel_size=5, stride=4, padding=2, активация ReLU`
- `Свёрточный слой 2: Conv2d(32 → 64), kernel_size=3, stride=1, padding=1, активация ReLU`
- `Flatten: преобразование 64×8×8 → 4096`
- `Выходной слой: Linear(4096 → 100)`

**Параметры обучения:**

- `optimizer = SGD(momentum=0.9)`
- `learning_rate = 0.005`
- `epochs = 500`
- `loss = CrossEntropyLoss`

![g](Screen/2.png)

_Рисунок 2: Графики изменения функции потерь и точности на эпохах_

Слева график показывает CCE (categorical cross-entropy) — это ошибка (loss) модели по эпохам: синяя линия (train)  уменьшается почти до нуля, значит модель всё хорошо запомнило обучающие данные; оранжевая линия (val) сначала тоже уменьшается, но затем начинает расти и сильнее колебаться, что обычно говорит о переобучении.

Справа график показывает Accuracy — это точность в процентах по эпохам: на обучении (синий) точность растёт почти до 100%, а на валидации (оранжевый) быстро поднимается примерно до 90–92% и дальше почти не улучшается, поэтому со временем появляется заметный разрыв между train и val

![result](Screen/3.png)

_Рисунок 3: Результаты_

### Часть 2. 

Следующим этапом нам нужно было провести тесты на 3 разных тактик пулинга

**Пуллинг за счёт шага свёртки (stride)**

В этом варианте пространственное разрешение уменьшается прямо внутри свёрточного слоя — достаточно взять stride > 1. Так мы одновременно извлекаем признаки и сжимаем размерность, можем сократить число слоёв и ускорить обучение. Минус в том, что при увеличенном шаге модель “просматривает” изображение реже, поэтому часть мелких деталей может теряться.

**Stride (шаг)** — это на сколько пикселей “сдвигается” окно свёртки/пулинга по высоте и ширине. Чем больше stride, тем сильнее уменьшается размер карты признаков.

- stride = 1 — окно свёртки/пулинга сдвигается на 1 пиксель, поэтому карта признаков почти не уменьшается (обычно сохраняется размер, если padding подобран).

- stride = 2 — сдвиг на 2 пикселя, из-за чего высота и ширина карты признаков обычно уменьшаются примерно в 2 раза (даунсэмплинг).

- stride = 3 — сдвиг на 3 пикселя, размер уменьшается ещё сильнее, модель быстрее теряет мелкие детали.

- stride = 4 (и больше) — агрессивное уменьшение разрешения: вычисления быстрее, но детализация заметно падает.

**Макс-пулинг (Max Pooling)**

Max-pooling — классический способ уменьшить размер карты признаков: из каждого локального окна берётся максимум. Это помогает выделять самые “сильные” признаки и делает модель более устойчивой к шуму и небольшим смещениям.

**Усредняющий пуллинг (Average Pooling)**

Average-pooling тоже уменьшает разрешение, но вместо максимума берёт среднее по окну. Он сильнее “сглаживает” информацию, снижает влияние шумов и лучше сохраняет общую структуру, но не так резко подчёркивает яркие признаки, как max-pooling.

![result](Screen/4.png)

_Рисунок 4: Результаты тестов_

После проведения тестов выяснилось, что вариант с усредняющим пулингом работает лучше: за счёт усреднения он сильнее сглаживает признаки, меньше реагирует на шум и в итоге лучше обобщает на новых данных.

### Вывод

В рамках лабораторной работы была реализована и обучена свёрточная нейронная сеть для классификации изображений из датасета CIFAR-100 по трём выбранным классам с использованием GPU. Также были экспериментально сравненены три способа уменьшения пространственного разрешения карт признаков: понижение размерности за счёт увеличенного шага свёртки (stride), максимальный пулинг (MaxPooling) и усредняющий пулинг (AvgPooling).

