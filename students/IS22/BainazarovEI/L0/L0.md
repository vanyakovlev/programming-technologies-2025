# Лабораторная работа №0. Установка локальной модели Qwen.

## Цель лабораторной работы

Установить на рабочую машину локальную модель нейросети Qwen и запустить её.

---

## Подготовка к выполнению задания

Перед началом работы необходимо было установить Python, WebUI для интеракций с языковой моделью и непосредственно саму модель Qwen. Для выполнения задания была использована версия Python 3.12.10, установлен text-generation-webui по ссылке https://github.com/oobabooga/text-generation-webui и модель Qwen Qwen2.5-Omni-3B-GGUF. После копирования text-generation-webui модель была добавлена в WebUI.

Чтобы запустить WebUI, была выполнена команда python server.py и был выполнен переход по ссылке (рис. 1). После запуска был открыт интерфейс WebUI (рис. 2). Далее во вкладке Model была выбрана и загружена модель Qwen.

<img width="974" height="131" alt="image" src="https://github.com/user-attachments/assets/a207fd1b-1974-433f-8ced-7a7f8a2457af" />

Рисунок 1 – Запуск WebUI

<img width="974" height="522" alt="image" src="https://github.com/user-attachments/assets/72b7fba3-3f7e-440c-a62e-ea00686c20bb" />

Рисунок 2 – Интерфейс WebUI

После загрузки модели, модель была протестирована с помощью простых промптов (рис. 3), также модель была протестирована с другими режимами диалога (chat, chat-instruct, instruct).
	
<img width="761" height="157" alt="image" src="https://github.com/user-attachments/assets/04f5d06a-fcf8-4530-83bf-a283e1714a92" />

Рисунок 3 – Тестирование модели

---

## Выполнение задания

### Задание 1

Далее по требованию первого задания был настроен системный промпт. Был выбран системный промпт Alpaca. Для этого во вкладке `Parameters >> Instruction templates`, а затем `Model >> Custom Instruction` Template был выбран соответствующий промпт (рис. 4-5). После этого модель с новым системным промптом также была протестирована (рис. 6).
	
<img width="961" height="516" alt="image" src="https://github.com/user-attachments/assets/8163b0ac-2e36-455a-b0f4-72d3eb5a8273" />

Рисунок 4 – Настройка системного промпта в Parameters.

<img width="632" height="306" alt="image" src="https://github.com/user-attachments/assets/9a2ef3ae-0d6d-4da5-90fd-bb2fbc5a7141" />

Рисунок 5 – Настройка системного промпта в Model.

<img width="849" height="286" alt="image" src="https://github.com/user-attachments/assets/d86b8909-e2de-4206-8e86-bb732a73f363" />

Рисунок 6 – Тестирование системного промпта Alpaca

### Задание 2

Вторым заданием было тестирование другой модели. В качестве другой модели была выбрана Qwen2.5-Omni-3B-Q8_0, которая также была протестирована (рис. 7).

<img width="783" height="955" alt="image" src="https://github.com/user-attachments/assets/658307d7-ceef-4073-bf90-bc5415f79f9c" />

Рисунок 7 – Тестирование модели Qwen2.5-Omni-3B-Q8_0

### Задание 3

Третьим заданием были настройки различных параметров (temperature, top_p, top_k, repetition_penalty, и т.д.). Эти настройки отвечают за случайность результатов:

* temperature: основной фактор, контролирующий случайность выходных данных. Модель выдает для следующего слова список всех возможных слов с их вероятностями. Параметр temperature применяется к этим вероятностям перед тем, как сделать окончательный выбор. При низкой температуре модель выбирает самые очевидные слова, а генерация становится более предсказуемой и сосредоточенной. Слова с высокой вероятностью становятся еще более вероятными, а слова с низкой — еще менее. При высокой вероятности все слова получают шанс быть выбранными. Генерация становится более случайной и разнообразной, но также может порождать бессмыслицу.
* top_p: выбирает самые вероятные токены, сумма вероятностей которых меньше значения top_p. 
* top_k: выбирает самые вероятные токены, количество которых равно значению top_k. 
* repetition_penalty: Штрафной коэффициент за повторение предыдущих токенов. Когда модель вычисляет вероятности для следующего слова, она искусственно понижает вероятность тех слов, которые уже встречались в предыдущем тексте. 1 означает отсутствие штрафа, Чем больше значение, тем меньше повторений.

Для выполнения задания модель была протестирована на разных значениях температуры. Действительно, при значении 1 модель при одном и том же запросе несколько раз подряд выдаёт разный текст, а при значении 0.01 (минимально возможное) - одинаковый (рис. 8-11). Если же выставить максимальное значение температуры (5) (рис. 12), модель действительно начнёт неадекватно себя вести и выводить бессмыслицу (рис. 13).

<img width="432" height="529" alt="{41F2AD14-ECF2-49E5-A534-EBFBE8327F06}" src="https://github.com/user-attachments/assets/461e0005-e1a7-47d0-a51f-8d6c556b3625" />

Рисунок 8 - Выставление значения температуры 1

<img width="874" height="890" alt="{AB42EC3C-5932-49EE-B952-FA4C9ADF817F}" src="https://github.com/user-attachments/assets/4e230d47-adad-48d6-b13a-e67de19764b6" />

Рисунок 9 - Наблюдение за изменением ответов при одном и том же запросе при температуре 1

<img width="434" height="535" alt="{846A297B-EC02-431B-9ADE-C5D2905CD45D}" src="https://github.com/user-attachments/assets/6bd5fa39-547f-48fd-bd38-e54cd23e549f" />

Рисунок 10 - Выставление минимального значения температуры 0.01

<img width="848" height="849" alt="{0490E60D-7254-4A60-A6F4-9D9C698ECFB9}" src="https://github.com/user-attachments/assets/8d484c4d-c778-44cf-850c-a7a2fcbe1e97" />

Рисунок 11 - Наблюдение за изменением ответов при одном и том же запросе при температуре 0.01

<img width="481" height="599" alt="image" src="https://github.com/user-attachments/assets/a1400b0a-e551-47f8-9c22-740ed2f98cc6" />

Рисунок 12 – Выставление максиммального значения температуры 5

<img width="965" height="358" alt="image" src="https://github.com/user-attachments/assets/b2e3e8e1-f1eb-4946-ac32-980b73a7eb96" />

Рисунок 13 – Неадекватное поведение модели при температуре 5

---

## Вывод

После выполнения лабораторной работы были освоены базовые навыки работы с локальными моделями. Была изучена работа с WebUI text-generation-webui и была проверена работа локальных моделей Qwen2.5-Omni-3B-GGUF и Qwen2.5-Omni-3B-Q8_0. Было изучено такое понятие, как системный промпт и были изучены различные параметры модели - temperature, top_p, top_k, repetition_penalty. Была проведено наблюдение над поведением модели при различных настройках температуры.

---
