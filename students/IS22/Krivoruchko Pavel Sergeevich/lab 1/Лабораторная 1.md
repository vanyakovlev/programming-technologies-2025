# Лабораторная работа №1

## Знакомство с OpenAI API. Написание простого текстового ассистента

**Цель работы:**
Научиться работать с OpenAI API, чтобы создать простого текстового ассистента, который может отвечать на наши вопросы.

## 1. Подготовка окружения

Сначала я создал виртуальное окружение, чтобы не засорять систему лишними библиотеками:

```bash
python -m venv venv
```

Потом активировал его:

```bash
venv\Scripts\activate
```

Установил нужные пакеты:

```bash
pip install openai python-dotenv
```

Далее создал файл `.env`, в котором хранится API-ключ и дополнительные параметры:

```plaintext
OPENAI_API_KEY=your-api-key-here
SYSTEM_PROMPT=Ты — дворовой гопник: говоришь грубо, с матом, но без злобы; не отвечаешь прямо, всё время съезжаешь с темы, вспоминаешь пацанов, просишь закурить или паспорт, шутишь по-уличному. Твоя речь — смесь уличного философствования и дворового базара, с жаргоном, наглостью и харизмой.
TEMPERATURE=0.7
```

Чтобы не залить ключ случайно в репозиторий, добавил `.env` в `.gitignore`.

Для фиксации зависимостей выполнил:

```bash
pip freeze > requirements.txt
```

## 2. Основной код ассистента

Создал файл `main.py` и написал в нём следующий код:

```python
import sys
from openai import OpenAI
from dotenv import load_dotenv
import os

load_dotenv()

api_key = os.getenv("OPENAI_API_KEY")
system_prompt = os.getenv("SYSTEM_PROMPT")
temperature = float(os.getenv("TEMPERATURE", 0.7))

client = OpenAI(api_key=api_key)

dialog_history = []

def get_response(user_text: str, dialog_history: list, client: OpenAI):
    dialog_history.append({"role": "user", "content": user_text})

    if len(dialog_history) > 6:
        dialog_history.pop(0)

    input_messages = [{"role": "system", "content": system_prompt}] + dialog_history

    response = client.responses.create(
        model="gpt-5-nano",
        input=input_messages,
        temperature=temperature
    )

    ai_message = response.output_text

    dialog_history.append({"role": "assistant", "content": ai_message})

    return ai_message

if __name__ == "__main__":
    print("Введите ваш вопрос (или 'exit' для выхода):")
    while True:
        question = input("Вы: ")
        if question.lower() == "exit":
            print("Завершение программы.")
            break
        answer = get_response(question, dialog_history, client)
        print("AI:", answer)
```

## 3. Как это работает

Когда программа запускается, она подгружает настройки из `.env` — ключ, температуру и системный промт.
Далее создаётся объект клиента `OpenAI`, через который и идут запросы к API.

Каждый раз, когда пользователь вводит сообщение, оно добавляется в историю.
Если история становится длиннее шести сообщений, старые реплики удаляются.
К модели уходит не просто текст, а весь контекст последних фраз, плюс системное сообщение, задающее стиль общения.

В моём случае системный промт выглядел так:

> “Ты — дворовой гопник: говоришь грубо, с матом, но без злобы; не отвечаешь прямо, всё время съезжаешь с темы, вспоминаешь пацанов, просишь закурить или паспорт, шутишь по-уличному. Твоя речь — смесь уличного философствования и дворового базара, с жаргоном, наглостью и харизмой.”

Из-за этого ассистент вёл себя максимально в образе — отвечал в стиле уличного персонажа, шутил, “философствовал” и даже иногда “просил прикурить”.

## 4. Эксперименты

### 1) Системный промт

C моим промтом диалог выглядел живее и даже немного комично — будто разговариваешь с персонажем из фильма “Бригада”.

![Описание картинки](pic/1,1.png)

![Описание картинки](pic/1,2.png)

![Описание картинки](pic/1,3.png)

### 2) Temperature

![Описание картинки](pic/t0.png)

При `temperature = 0` ответы становились предсказуемыми, почти одинаковыми.

![Описание картинки](pic/t1.png)

При `temperature = 1` баланс нормальный — модель немного импровизирует, но не уходит в бред.

![Описание картинки](pic/t1,5.png)

При `temperature = 1.5` модель начинает уезжать в полный бред: путает темы, придумывает несвязанные истории, может «галлюцинировать».

![Описание картинки](pic/t2.png)

А если ставить `1.8` и выше, ассистент выдаёт совершенно несвязный набор букв, символов и фрагментов; из такого результата нельзя получить никакой связной информации.

### 3) История диалога

![Описание картинки](pic/ex3,1.png)

![Описание картинки](pic/ex3,2.png)

Добавление короткой памяти (по 3 сообщения от каждой стороны) дало ощущение, что ассистент реально “помнит”, о чём говорили.
Например, если спросить про “стрельцов с оригами”, он потом мог вспомнить это в следующих ответах.

## 6. Вывод

В результате работы получился консольный ассистент, который умеет:

- подключаться к OpenAI API;
- использовать системный промт для выбора “характера” модели;
- регулировать температуру генерации;
- запоминать последние реплики диалога;
- и, самое интересное, — отвечать в заданном образе (в моём случае — “дворового гопника”).

Такой подход показал, как гибко можно настраивать поведение модели, просто меняя промт и параметры.
Сам код короткий, но функциональный, и может быть базой для более сложных чат-ботов.
