# Министерство науки и высшего образования РФ ФГБОУ ВО Заполярный государственный институт имени Н.М.Федоровского

## Технологии программирования. Лабораторная работа №0 Тема: «Установка локальной модели Qwen»

_Работу выполнил:_

_Студент группы ИС-22_

_Шелепов Денис Владимирович_

_Работу проверил:_

_Сидельников Максим Эдуардович_

_Дата выполнения работы: 20.10.2025_

### Цель:

Установить на рабочую машину локальную модель нейросети Qwen и запустить её.

### План

- Настройка окружения;
- Запуск языковой модели;
- Задания.

## Ход работы

Для выполнения лабораторной работы взамен модели Qwen была использована модель Meta-Llama-3.1-8B-Instruct-Q5_K_M, ввиду характеристик имеющейся видеокарты (Nvidea GForce RTX 3050 8Gb). Данная модель успешно выполняла поставленные задачи.

В рамка проведения работы было составлено 2 промпта, регулирующих поведение и стилистику речи модели.

Первый промпт:

> [Роль: Сеньор JavaScript разработчик] > [Стек: JavaScript, TypeScript, PostgreSQL] > [Уровень: Эксперт с 10+ годами опыта] > [Стиль объяснений: Детальный, практико-ориентированный, с акцентом на лучшие практики] > [Особенность: В каждом ответе добавляет один дополнительный пример из JS/TS для лучшего понимания] > [Цель: Давать максимально полезные и применимые на практике объяснения]

Данный промпт оказался для модели легким

![JS_Senior](Images/4.png)

Как видно из изображения модель справилась с поставленной характеристикой. В качестве ответа на вопрос "Что такое PARTITION BY?" модель дала ответ на яхыке программирования TS, несмотря на то что PARTITION BY является аргументом оконной функции в SQL.

Второй промпт:

> [Роль: Рик Санчез] > [Стиль речи: циничный, саркастичный, отрывистый, с использованием научного жаргона и ненормативной лексики (замены: "блин", "черт")] > [Ключевые характеристики: постоянно употребляет "Мы", начинает ответы с "Бурп...", использует культовые фразы ("Wubba Lubba Dub Dub", "Соберись, тряпка"), презирает сентиментальность, говорит быстро и невнятно, с паузами (...), обрывает предложения] > [Личность: самый умный человек в мультивселенной, алкоголик, социопат, скрывающий травмы за грубостью]
> Твоя задача: отвечать на все вопросы так, как это сделал бы Рик Санчез.

![alt text](Images/3.png)

![alt text](Images/2.png)

![alt text](Images/1.png)

Как представленно на изображениях, с данной задачей модель от Meta так же справилась, пусть и не точно, повторяя повадки персанажа Рика Санчеза из мультсериала "Рик и Морти"

Для сравнения, пример работы с промптом от нейросети Deep Seek
![alt text](Images/5.png)

## Доп. исследования

Для проведения дополнительных экспериментов была избарана модель Qwen2.5-Omni-3B-UD-Q8_K_XL.gguf.

Параметры для сревнения были выставлены следующие: temperature = 1, min_p = 0.95, top_p = 0.7, top_k = 120. Доступ в интернет был оключен. Промпт: JS_Senior

---

### Qwen

![alt text](Images/6.png)

---

### Meta

![alt text](Images/8.png)

![alt text](Images/7.png)

---

При сравнении моделей замечена явная разница в "понимании" и исполнении промпта. Модель meta справилась с этой задачей с результатом лучше по сравнению с qwen несмотря на идентичность промпта и параметров.
Модель meta привела пример совпадаюзий с промптом.
Подобное поведение было очевидно в виду значительной разницы в числе обучаемых параметро. Рзаница в 5 миллиардов в пользу meta.

## Параметры модели

### Temperature

Temperature контролирует случайность текста, генерируемого LLM во время вывода. Путем настроек и проведений тество было выявлено оптимальное значение. От 0.8 до 1.5. В пределах этих значений сгенерированный текст был логичен и понятен при чтении

### min_p

Выбираются токены которые больше или равны пороговому значению.
Порог = вероятность токена \* min_p

### top_p

Суммируюутся самые вероятные токены до тех пор пока сумма не станет больше или равна p
Выборка идет из токенов, вероятность которых попала в суммирование

### top_k

Выбор k самых вероятных токенов. Идет ограничение по количеству или же словарному запасу ответа модели

## Заключение

В ходе выполнения лабораторной работы было изучено средство запуска ИИ моделей формата .gguf - формат файла для llama.cpp. Произведены настройки и изучены ключевые особенности конфигурации, влияющие на ответы сети. Так же замечены различия, указываемые в названиях моделей, а именно количественные значения обучаемых параметров.
