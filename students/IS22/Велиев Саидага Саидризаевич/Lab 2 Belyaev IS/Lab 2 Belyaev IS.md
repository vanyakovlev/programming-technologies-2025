# Отчёт. Лабораторная работа №2.

### **1. Подготовка данных и настройка окружения**

Первым шагом был выбор конкретных классов из датасета CIFAR-100. Это необходимо для упрощения задачи бинарной классификации.

```python

CLASSES = [22, 11]

mask = np.isin(train_y, CLASSES) 
train_X = train_X[mask].copy()  
train_y = train_y[mask].copy()  

train_y = np.unique(train_y, return_inverse=1)[1]
```
**Настройка GPU** была критически важна, так как обучение нейронных сетей на CPU занимает в десятки раз больше времени. Команда `!nvidia-smi` подтвердила доступность GPU Tesla T4 в среде Colab. Все вычисления были перенесены на GPU через `.to(device)`.

### **2. Архитектура модели и реализация трех типов пуллинга**

Была спроектирована CNN-архитектура с модульным выбором типа пуллинга. Это позволило провести "честное" сравнение, изменяя только один компонент системы.

```python
class Cifar100_CNN(nn.Module):
    def __init__(self, pooling_type='max'):  # Параметр по умолчанию

        if pooling_type == 'stride':
            self.pool = nn.Conv2d(..., stride=2, ...)
        elif pooling_type == 'max':
            self.pool = nn.MaxPool2d(2)
        elif pooling_type == 'avg':
            self.pool = nn.AvgPool2d(2)
```

### **Общая концепция пуллинга (Pooling)**

**Что такое пуллинг и зачем он нужен?**
Пуллинг (объединение) — это операция **уменьшения пространственных размеров** карт признаков (feature maps) в сверточных нейронных сетях. Основные цели:

1.  **Уменьшение вычислительной сложности** — меньше параметров для обучения.
2.  **Контроль переобучения** — создание инвариантности к малым смещениям и искажениям.
3.  **Выделение наиболее важных признаков** из локальных областей.

**Max Pooling**

Максимальное объединение выбирает нейроны с максимальной активацией в окне, что позволяет модели сосредотачиваться на самых важных признаках. Это создает трансляционную инвариантность, то есть объект может немного смещаться в пределах окна и всё равно будет распознан. Однако, этот метод может терять пространственную информацию и быть чувствительным к шуму.

**Average Pooling**

Среднее объединение усредняет значения в окне, что сглаживает данные и уменьшает влияние выбросов. Это помогает сохранять общую картину, улучшая стабильность обучения, особенно в глубоких сетях. Но этот метод может размывать четкие границы и снижать контрастность признаков, что иногда ухудшает результат.

**Stride Pooling**

Пулинг через шаг свертки использует обучаемые параметры (веса и смещения), что позволяет модели самостоятельно решать, как лучше уменьшать размерность данных. Этот метод сохраняет больше информации по сравнению с Max Pooling, так как использует линейную комбинацию признаков. Однако, это требует большего количества параметров, что делает модель вычислительно дороже и может привести к переобучению.



**Архитектурные улучшения для повышения точности:**

1.  **Batch Normalization** (`nn.BatchNorm2d`) — стабилизирует распределение активаций между слоями, ускоряет сходимость и позволяет использовать более высокий learning rate.
2.  **Dropout** (`nn.Dropout(0.3)`) — случайным образом "отключает" 30% нейронов во время обучения, что предотвращает запоминание (переобучение) и улучшает обобщающую способность.
3.  **Глобальный средний пулинг** (`nn.AdaptiveAvgPool2d(1)`) — заменяет финальный полносвязный слой для уменьшения пространства признаков, что снижает количество параметров и риск переобучения.

### **3. Процесс обучения и валидации**
Для каждой модели (stride, max, avg) запускался идентичный процесс обучения с контролем переобучения.

```python
for epoch in range(EPOCHS):
    model.train()
    for batch in dataloader['train']:
        outputs = model(inputs)
        loss = criterion(outputs, labels)
        
        loss.backward()

        optimizer.step()
    

    model.eval() 
    with torch.no_grad(): 
        for batch in dataloader['test']:
            val_outputs = model(val_inputs)
            val_loss = criterion(val_outputs, val_labels)
```

**Ключевые моменты:**
- **Разделение на train/test** (80%/20%) обеспечивает объективную оценку способности модели обобщать знания на новые данные.
- **Контроль переобучения** — разница между точностью на тренировочной и валидационной выборках (например, 0.977 vs 0.735 у max pooling) показывает, насколько модель "запомнила" данные вместо их понимания.
- **Ранняя остановка (в логике)** — сохранение модели с лучшей валидационной точностью (`best_val_acc: 0.8950`) предотвращает сохранение переобученной версии.

### **4. Детальный анализ результатов обучения**
**Результаты обучения:**

| Метрика | Stride Pooling | Max Pooling | Avg Pooling | **Лучший** |
|---------|---------------|-------------|-------------|------------|
| **Лучшая Val Accuracy** | 0.8750 | **0.8950** | 0.8850 | **Max** |
| **Final Train Accuracy** | 0.9790 | 0.9770 | 0.9300 | Stride |
| **Final Val Accuracy** | 0.8350 | 0.7350 | 0.6100 | Stride |
| **Степень переобучения** (Train-Val) | 0.1440 | **0.2420** | 0.3200 | **Stride** |
| **Время обучения (сек)** | 31.36 | **21.45** | 21.33 | **Avg/Max** |
| **Тестовая Accuracy** | *не вычислялась* | **0.7350** | *не вычислялась* | **Max** |

**Интерпретация результатов:**

1.  **Stride Pooling (пулинг через шаг свертки):**
    - **Преимущество**: Наименьшее переобучение (0.1440) → лучшая обобщающая способность.
    - **Недостаток**: Самое долгое обучение (31.36с), так как операция свертки с stride=2 вычислительно тяжелее простого выбора максимума.
    - **Механика**: Уменьшает размерность, сохраняя возможность обучения параметров ядра свертки.

2.  **Max Pooling (максимальный пулинг):**
    - **Преимущество**: Наивысшая пиковая точность (0.8950) и быстрое обучение.
    - **Главный недостаток**: Наибольшее переобучение (0.2420) → на тренировочных данных точность 97.7%, но на новых — только 73.5%.
    - **Причина**: Выбор максимума создает разреженные активации, которые могут чрезмерно специфичны для тренировочных данных.

3.  **Average Pooling (усредняющий пулинг):**
    - **Преимущество**: Быстрейшее обучение и сглаживание признаков.
    - **Недостаток**: Наихудшая итоговая точность на валидации (падение до 0.6100 к 50-й эпохе).
    - **Причина**: Усреднение "размывает" важные особенности, делая признаки менее discriminative.

## Итоговое заключение:
Проведено сравнение трех методов пулинга с анализом точности, времени обучения и степени переобучения. Лучшей признана модель с **Max Pooling**, сохранена в форматах PyTorch и ONNX. Полученные результаты коррелируют с теорией: Max Pooling дает высшую точность, но требует методов регуляризации для контроля переобучения.
