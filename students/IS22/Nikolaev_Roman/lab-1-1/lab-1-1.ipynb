{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f61925d",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mДля выполнения ячеек с \"Python 3.12.10\" требуется пакет ipykernel.\n",
      "\u001b[1;31m<a href='command:jupyter.createPythonEnvAndSelectController'>Создать среду Python</a> с необходимыми пакетами.\n",
      "\u001b[1;31mИли установите \"ipykernel\" с помощью следующей команды: \"\"c:/Program Files/Python312/python.exe\" -m pip install ipykernel -U --user --force-reinstall\""
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import pickle\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.datasets import make_circles, make_moons\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8567d181",
   "metadata": {},
   "source": [
    "# Часть 1. Задача регрессии по теореме универсальной аппроксимации, ручное дифференцирование\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "614747d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = (np.arange(100)/100 - 0.5).repeat(5)\n",
    "\n",
    "y = 1/(1+np.exp(-10*X))\n",
    "yn = np.random.normal(scale=0.05, size=y.size)+y\n",
    "\n",
    "plt.plot(X, yn)\n",
    "plt.plot(X, y, linestyle='--', c='k')\n",
    "################################################\n",
    "tensor_X = torch.Tensor(X.reshape(-1, 1))\n",
    "tensor_y = torch.Tensor(yn.reshape(-1, 1))\n",
    "\n",
    "HIDDEN_SIZE = 64\n",
    "# Инициализация весов MLP с одним скрытым слоём\n",
    "weights_1 = (torch.rand(1, HIDDEN_SIZE)-.5)/10\n",
    "bias_1 = torch.zeros(HIDDEN_SIZE)\n",
    "\n",
    "weights_2 = (torch.rand(HIDDEN_SIZE, 1)-.5)/10\n",
    "bias_2 = torch.zeros(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e5cb134",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Определяем функцию нелинейности\n",
    "relu = lambda x: torch.maximum(x, torch.Tensor([0]))\n",
    "# Прямой проход\n",
    "forward = lambda x: (weights_2.t()*relu((weights_1*x) + bias_1)\n",
    "                      ).sum(axis=-1,keepdims=True) + bias_2\n",
    "loss = lambda y, y_: ((y-y_)**2).sum(axis=-1)\n",
    "\n",
    "# обратный проход\n",
    "def backward(X, y, y_pred):\n",
    "    # производная функции потерь по y_pred\n",
    "    dL = 2*(y_pred-y)\n",
    "    # значения нейронов скрытого слоя до применения активации\n",
    "    Ax = (weights_1*X) + bias_1\n",
    "    # значения нейронов скрытого слоя после применения активации\n",
    "    A = relu(Ax)\n",
    "    # производная функции потерь по weight_2\n",
    "    dW2 = torch.mm(A.t(), dL)\n",
    "    # производная функции потерь по bias_2\n",
    "    db2 = dL.sum(axis=0)\n",
    "    # производная функции потерь по значениям скрытого слоя после активации\n",
    "    dA = torch.mm(dL, weights_2.t())\n",
    "    # производная функции потерь по значениям скрытого слоя до активации\n",
    "    dA[Ax<=0] = 0\n",
    "    # производная функции потерь по weight_1\n",
    "    dW = torch.mm(X.t(), dA)\n",
    "    # производная функции потерь по bias_1\n",
    "    db = dA.sum(axis=0)\n",
    "    #print(dW.shape, db.shape, dW2.shape, db2.shape)\n",
    "    return dW, db, dW2, db2\n",
    "\n",
    "def optimize(params, grads, lr=0.001):\n",
    "    # градиентный спуск по всей обучающей выборке\n",
    "    W1, b1, W2, b2 = params\n",
    "    W1 -= lr*grads[0]\n",
    "    W2 -= lr*grads[2]\n",
    "    b1 -= lr*grads[1]\n",
    "    b2 -= lr*grads[3]\n",
    "    return W1, b1, W2, b2\n",
    "\n",
    "for i in range(50000): # 50 тысяч итераций градиентного спуска == 50 тысяч эпох\n",
    "  output = forward(tensor_X)\n",
    "  cur_loss = loss(output, tensor_y)\n",
    "  grads = backward(tensor_X, tensor_y, output)\n",
    "  params = [weights_1, bias_1, weights_2, bias_2]\n",
    "  weights_1, bias_1, weights_2, bias_2 = optimize(params, grads, 1e-4)\n",
    "  if (i+1)%10000 == 0:\n",
    "      plt.plot(X, output.numpy(), label=str(i+1), alpha=0.5)\n",
    "plt.plot(X, y, linestyle='--', c='k', label='real')\n",
    "plt.legend()\n",
    "plt.ylim(y.min(), y.max())\n",
    "print(cur_loss.numpy().mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b46015a9",
   "metadata": {},
   "source": [
    "# Часть 2. Бинарная классификация с помощью автодиффиренцирования PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0839ddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.random.randint(2, size=(1000, 2))\n",
    "\n",
    "y = (X[:, 0] + X[:, 1]) % 2 # XOR\n",
    "X = X + np.random.normal(0, scale=0.1, size=X.shape)\n",
    "#X, y = make_circles(n_samples=1000, noise=0.025)\n",
    "#X, y = make_moons(n_samples=1000, noise=0.025)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y)\n",
    "####################################################\n",
    "tensor_X = torch.Tensor(X.reshape(-1, 2))\n",
    "tensor_y = torch.Tensor(y.reshape(-1, 1))\n",
    "\n",
    "HIDDEN_SIZE = 16\n",
    "# Инициализация весов MLP с одним скрытым слоём\n",
    "weights_1 = ((torch.rand(2, HIDDEN_SIZE)-.5)/10).detach().requires_grad_(True)\n",
    "bias_1 = torch.zeros(HIDDEN_SIZE, requires_grad=True)\n",
    "\n",
    "weights_2 = ((torch.rand(HIDDEN_SIZE, 1)-.5)/10).detach().requires_grad_(True)\n",
    "bias_2 = torch.zeros(1, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0c144e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Определяем функцию нелинейности\n",
    "def sigmoid(x):\n",
    "    return 1/(1+torch.exp(-x))\n",
    "\n",
    "# Прямой проход\n",
    "def forward(x):\n",
    "    hidden = torch.mm(x, weights_1) + bias_1\n",
    "    hidden_nonlin = sigmoid(hidden)\n",
    "    output = (weights_2.t()*hidden_nonlin).sum(axis=-1,keepdims=True) + bias_2\n",
    "    return sigmoid(output)\n",
    "\n",
    "# Logloss\n",
    "def loss(y_true, y_pred):\n",
    "    return -1*(y_true*torch.log(y_pred)+(1-y_true)*torch.log(1-y_pred)).sum()\n",
    "\n",
    "# задаём шаг обучения\n",
    "lr = 1e-3\n",
    "# задаём число итераций\n",
    "iters = 10000\n",
    "params = [weights_1, bias_1, weights_2, bias_2]\n",
    "losses = []\n",
    "for i in range(iters):\n",
    "    output = forward(tensor_X)\n",
    "    lossval = loss(tensor_y, output)\n",
    "    lossval.backward() # тут включается в работу autograd\n",
    "    for w in params:\n",
    "        with torch.no_grad():\n",
    "            w -= w.grad*lr # обновляем веса\n",
    "        w.grad.zero_() # зануляем градиенты, чтобы не накапливались за итерации\n",
    "    losses.append(lossval.item())\n",
    "# выводим историю функции потерь по итерациям\n",
    "plt.plot(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2408e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_diff = X.max() - X.min()\n",
    "X_left = X.min() - 0.1*X_diff\n",
    "X_right = X.max() + 0.1*X_diff\n",
    "grid = np.arange(X_left, X_right, 0.01)\n",
    "grid_width = grid.size\n",
    "surface = []\n",
    "# создаем точки по сетке\n",
    "for x1 in grid:\n",
    "    for x2 in grid:\n",
    "        surface.append((x1, x2))\n",
    "surface = np.array(surface)\n",
    "# получаем предсказания для всех точек\n",
    "with torch.no_grad():\n",
    "    Z = forward(torch.Tensor(surface)).detach().numpy()\n",
    "# меняем форму в виде двухмерного массива\n",
    "Z = Z.reshape(grid_width, grid_width)\n",
    "xx = surface[:, 0].reshape(grid_width, grid_width)\n",
    "yy = surface[:, 1].reshape(grid_width, grid_width)\n",
    "# рисуем разделяющие поверхности классов\n",
    "plt.contourf(xx, yy, Z, alpha=0.5)\n",
    "# рисуем обучающую выборку\n",
    "plt.scatter(X[:, 0], X[:, 1], c=output.detach().numpy()>0.5)\n",
    "# задаём границы отображения графика\n",
    "plt.xlim(X_left, X_right)\n",
    "plt.ylim(X_left, X_right)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7920372e",
   "metadata": {},
   "source": [
    "# Часть 3. Классификация изображений CIFAR100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb7eb2c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/cifar-100-python/train', 'rb') as f:\n",
    "    data_train = pickle.load(f, encoding='latin1')\n",
    "with open('data/cifar-100-python/test', 'rb') as f:\n",
    "    data_test = pickle.load(f, encoding='latin1')\n",
    "\n",
    "# Здесь указать ваши классы по варианту!!!\n",
    "CLASSES = [0, 33, 50]\n",
    "\n",
    "train_X = data_train['data'].reshape(-1, 3, 32, 32)\n",
    "train_X = np.transpose(train_X, [0, 2, 3, 1]) # NCHW -> NHWC\n",
    "train_y = np.array(data_train['fine_labels'])\n",
    "mask = np.isin(train_y, CLASSES)\n",
    "train_X = train_X[mask].copy()\n",
    "train_y = train_y[mask].copy()\n",
    "train_y = np.unique(train_y, return_inverse=1)[1]\n",
    "del data_train\n",
    "\n",
    "test_X = data_test['data'].reshape(-1, 3, 32, 32)\n",
    "test_X = np.transpose(test_X, [0, 2, 3, 1])\n",
    "test_y = np.array(data_test['fine_labels'])\n",
    "mask = np.isin(test_y, CLASSES)\n",
    "test_X = test_X[mask].copy()\n",
    "test_y = test_y[mask].copy()\n",
    "test_y = np.unique(test_y, return_inverse=1)[1]\n",
    "del data_test\n",
    "Image.fromarray(train_X[123]).resize((256,256))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb1cd64e",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "dataloader = {}\n",
    "for (X, y), part in zip([(train_X, train_y), (test_X, test_y)],\n",
    "                        ['train', 'test']):\n",
    "    tensor_x = torch.Tensor(X)\n",
    "    tensor_y = F.one_hot(torch.Tensor(y).to(torch.int64),\n",
    "                                     num_classes=len(CLASSES))/1.\n",
    "    dataset = TensorDataset(tensor_x, tensor_y) # создание объекта датасета\n",
    "    dataloader[part] = DataLoader(dataset, batch_size=batch_size, shuffle=True) # создание экземпляра класса DataLoader\n",
    "dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12ead6a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Normalize(nn.Module):\n",
    "    def __init__(self, mean, std):\n",
    "        super(Normalize, self).__init__()\n",
    "        self.mean = torch.tensor(mean)\n",
    "        self.std = torch.tensor(std)\n",
    "\n",
    "    def forward(self, input):\n",
    "        x = input / 255.0\n",
    "        x = x - self.mean\n",
    "        x = x / self.std\n",
    "        return torch.flatten(x, start_dim=1) # nhwc -> nm\n",
    "\n",
    "class Cifar100_MLP(nn.Module):\n",
    "    def __init__(self, hidden_size=32, classes=100):\n",
    "        super(Cifar100_MLP, self).__init__()\n",
    "        # https://blog.jovian.ai/image-classification-of-cifar100-dataset-using-pytorch-8b7145242df1\n",
    "        self.norm = Normalize([0.5074,0.4867,0.4411],[0.2011,0.1987,0.2025])\n",
    "        self.seq = nn.Sequential(\n",
    "            nn.Linear(32*32*3, 20), # Первый слой (пошире)\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(20, 12),      # Второй слой\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(12, classes)   # Выход\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        x = self.norm(input)\n",
    "        return self.seq(x)\n",
    "\n",
    "HIDDEN_SIZE = 128\n",
    "model = Cifar100_MLP(hidden_size=HIDDEN_SIZE, classes=len(CLASSES))\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc203042",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c1440cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 30\n",
    "steps_per_epoch = len(dataloader['train'])\n",
    "steps_per_epoch_val = len(dataloader['test'])\n",
    "for epoch in range(EPOCHS):  # проход по набору данных несколько раз\n",
    "    running_loss = 0.0\n",
    "    model.train()\n",
    "    for i, batch in enumerate(dataloader['train'], 0):\n",
    "        # получение одного минибатча; batch это двуэлементный список из [inputs, labels]\n",
    "        inputs, labels = batch\n",
    "\n",
    "        # очищение прошлых градиентов с прошлой итерации\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # прямой + обратный проходы + оптимизация\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        #loss = F.cross_entropy(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # для подсчёта статистик\n",
    "        running_loss += loss.item()\n",
    "    print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / steps_per_epoch:.3f}')\n",
    "    running_loss = 0.0\n",
    "    model.eval()\n",
    "    with torch.no_grad(): # отключение автоматического дифференцирования\n",
    "        for i, data in enumerate(dataloader['test'], 0):\n",
    "            inputs, labels = data\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            running_loss += loss.item()\n",
    "    print(f'[{epoch + 1}, {i + 1:5d}] val loss: {running_loss / steps_per_epoch_val:.3f}')\n",
    "print('Обучение закончено')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aef2c6e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for part in ['train', 'test']:\n",
    "    y_pred = []\n",
    "    y_true = []\n",
    "    with torch.no_grad(): # отключение автоматического дифференцирования\n",
    "        for i, data in enumerate(dataloader[part], 0):\n",
    "            inputs, labels = data\n",
    "\n",
    "            outputs = model(inputs).detach().numpy()\n",
    "            y_pred.append(outputs)\n",
    "            y_true.append(labels.numpy())\n",
    "        y_true = np.concatenate(y_true)\n",
    "        y_pred = np.concatenate(y_pred)\n",
    "        print(part)\n",
    "        print(classification_report(y_true.argmax(axis=-1), y_pred.argmax(axis=-1),\n",
    "                                    digits=4, target_names=list(map(str, CLASSES))))\n",
    "        print('-'*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ad8e562",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = list(model.parameters())[0].detach().numpy()\n",
    "print(weights.shape)\n",
    "fig, ax = plt.subplots(1, weights.shape[0], figsize=(3*weights.shape[0], 3))\n",
    "for i, ω in enumerate(weights):\n",
    "    ω = ω.reshape(32, 32, 3)\n",
    "    ω -= np.percentile(ω, 1, axis=[0, 1])\n",
    "    ω /= np.percentile(ω, 99, axis=[0, 1])\n",
    "    ω = np.clip(ω, 0, 1)\n",
    "    ax[i].imshow(ω)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
