# Вопросы к экзамену по дисциплине "Технологии программирования"

# Блок 1. Архитектура и принципы работы LLM
1. В чём принципиальное отличие autoregressive-моделей от encoder-based моделей?
2. Почему трансформеры вытеснили RNN и LSTM в задачах обработки текста?
3. Что такое контекстное окно модели и какие факторы ограничивают его размер?
4. Почему увеличение числа параметров модели не всегда приводит к улучшению качества ответов?
5. Что такое галлюцинации языковой модели и почему они возникают на архитектурном уровне?
6. В чём разница между instruction-tuned и base моделями?
7. Какую роль играет системный промпт в формировании распределения вероятностей токенов?

# Блок 2. Локальные модели и облачные API
8. Архитектурные преимущества и недостатки локального запуска LLM.
9. Что есть процесс квантования языковой модели?
10. Почему квантование модели влияет на скорость и качество генерации?
11. Как ограничение VRAM влияет на выбор архитектуры модели?
12. Почему локальные модели сложнее масштабировать, чем API-сервисы?
13. В каких сценариях локальная модель предпочтительнее с точки зрения безопасности?

# Блок 3. Параметры генерации и теория вероятностей
14. Что такое temperature с точки зрения распределения вероятностей?
15. Почему при temperature = 0 модель становится детерминированной?
16. Как работает top-p и чем он отличается от top-k?
17. Почему некоторые комбинации top-k и top-p могут давать нестабильные результаты?
18. Как repetition penalty влияет на энтропию выходного текста?
19. Почему нельзя подобрать идеальные параметры генерации для всех задач?
20. Почему разные модели по-разному реагируют на одинаковые параметры?

# Блок 4. API, контекст и управление диалогом
21. Что такое токенизация и почему разные модели считают токены по-разному?
22. Как контекст диалога влияет на стоимость и латентность API-запросов?
23. Какие стратегии усечения контекста используются в продакшене?
24. Почему контекст ≠ память с точки зрения модели?
25. Какие риски возникают при передаче чувствительных данных в LLM-API?
26. Как rate limiting влияет на архитектуру клиентских приложений?
27. Почему асинхронные клиенты критичны для high-load сценариев?
28. Почему LLM плохо подходят для задач с жёсткими логическими ограничениями?

# Блок 5. Векторные представления и семантический поиск
29. Что такое embedding-пространство и какие свойства оно должно иметь?
30. Почему косинусное расстояние чаще используется для текстовых embeddings?
31. В чём принципиальная разница между keyword search и semantic search?
32. Почему размер чанка влияет на precision и recall поиска?
33. Как overlap между чанками влияет на связность контекста?
34. Как индексирование в Milvus ускоряет поиск в высоких размерностях?
35. Почему векторные БД плохо подходят для точных числовых запросов?
36. Какие архитектурные проблемы возникают при росте объёма embedding-данных?
37. Почему связка LLM + Vector DB (RAG) считается компромиссом, а не идеальным решением?